**************************************************************************
******* 1_Cluster_Installation_using_Kubeadm *****************************
**************************************************************************
-----------------------------------------------------------------------------



1. Install the kubeadm and kubelet packages on the controlplane and node01 nodes.
Use the exact version of 1.31.0-1.1 for both.

kubeadm installed on controlplane?
kubelet installed on controlplane?
Kubeadm installed on worker node01?
Kubelet installed on worker node01?
These steps have to be performed on both nodes. set net.bridge.bridge-nf-call-iptables to 1:

Run 2nd terminal ---> ssn node01 and do below commands for these 2 servers:
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system
------------------------------
The container runtime has already been installed on both nodes, so you may skip this step.
Install kubeadm, kubectl and kubelet on all nodes:

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update

# To see the new version labels
sudo apt-cache madison kubeadm kubelet kubectl
sudo apt-get install -y kubelet=1.31.0-1.1 kubeadm=1.31.0-1.1 kubectl=1.31.0-1.1
sudo apt-mark hold kubelet kubeadm kubectl

kubeadm version -----> Major:"1", Minor:"31" ---> 1.31
kubelet --version ---> v1.31.0
kubectl version -----> v1.31.0, The connection to the server localhost:8080 was refused - did you specify the right host or port?
------------------------------------------------------------------------------------


2. What is the version of kubelet installed?
node01 ~ ✖ kubelet --version
Kubernetes v1.31.0
------------------------------------------------------------------------------------


3. How many nodes are part of kubernetes cluster currently?
Are you able to run kubectl get nodes?  ---> NO ---> 0
------------------------------------------------------------------------------------

4. Lets now bootstrap a kubernetes cluster using kubeadm. ---> ok
The latest version of Kubernetes will be installed.
------------------------------------------------------------------------------------

5. Initialize Control Plane Node (Master Node). Use the following options:

5.1. apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node
     apiserver-cert-extra-sans - Set it to controlplane
     pod-network-cidr - Set to 10.244.0.0/16
Once done, set up the default kubeconfig file and wait for node to be part of the cluster.
Wish = Controlplane node initialized


5.1 controlplane ~ ✖ ip a show eth0 ---> inet 192.6.92.3/24 brd 192.6.92.255 scope global eth0
    kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address 192.6.92.3 --pod-network-cidr=10.244.0.0/16

Your Kubernetes control-plane has initialized successfully!
To start using your cluster, you need to run the following as a regular user:
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
Alternatively, if you are the root user, you can run:
  export KUBECONFIG=/etc/kubernetes/admin.conf
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/
Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 192.6.92.3:6443 --token 6f5wlg.ws2ot39w4yuach85 \
        --discovery-token-ca-cert-hash sha256:4ed97d1ce8940bbeef070481ed6a6db79a6121438f21f17ca24ac09b90fb099f 

controlplane ~ ➜  mkdir -p $HOME/.kube
controlplane ~ ➜  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
controlplane ~ ➜  sudo chown $(id -u):$(id -g) $HOME/.kube/config
controlplane ~ ➜  kubectl get nodes
NAME           STATUS     ROLES           AGE     VERSION
controlplane   NotReady   control-plane   2m42s   v1.31.0
-------------------------------------------------------------------------------------------------------------------------

You can use the below kubeadm init command to spin up the cluster:


IP_ADDR=$(ip addr show eth0 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
kubeadm init --apiserver-cert-extra-sans=controlplane --apiserver-advertise-address $IP_ADDR --pod-network-cidr=10.244.0.0/16
Once you run the init command, you should see an output similar to below:
[init] Using Kubernetes version: v1.31.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0529 15:35:11.112522   11469 checks.go:844] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm.It is recommended to use "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.133.43.3]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [192.133.43.3 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [192.133.43.3 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 553.930452ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 12.503398796s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node controlplane as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 90l0iw.8sa7trjypfybs5l1
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.133.43.3:6443 --token 90l0iw.8sa7trjypfybs5l1 \
        --discovery-token-ca-cert-hash sha256:a3793ea96e136d50cb06a5f380c134d00f3f9596a28ffb1dce110995eb29ea4d 


Once the command has been run successfully, set up the kubeconfig:

mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config
------------------------------------------------------------------------------------

6. Generate a kubeadm join token Or copy the one that was generated by kubeadm init command -> done in previous step
------------------------------------------------------------------------------------

7. Join node01 to the cluster using the join token. Node01 joined the cluster? 
Use the join token provided by the kubeadm command or create a new token.
--- if need
kubeadm token create --print-join-command
kubeadm join 192.133.43.3:6443 --token b68tyg.8qtnz7pwe7zqs4fi --discovery-token-ca-cert-hash sha256:a3793ea96e136d50cb06a5f380c134d00f3f9596a28ffb1dce110995eb29ea4d
-----------

controlplane ~ ➜  ssh node01 and copy command  from controlplane
Last login: Mon Jan 13 08:23:46 2025 from 192.6.92.4

node01 ~ ✖ kubeadm join 192.6.92.3:6443 --token 6f5wlg.ws2ot39w4yuach85         --discovery-token-ca-cert-hash sha256:4ed97d1ce8940bbeef070481ed6a6db79a6121438f21f17ca24ac09b90fb099f
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.001370983s
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

---> controlplane ~ ➜  kubectl get nodes
NAME           STATUS     ROLES           AGE     VERSION
controlplane   NotReady   control-plane   9m37s   v1.31.0
node01         NotReady   <none>          58s     v1.31.0
------------------------------------------------------------------------------------

8. To install a network plugin, we will go with Flannel as the default choice. For inter-host communication, we will utilize the eth0 interface.
Please ensure that the Flannel manifest includes the appropriate options for this configuration.
Refer to the official documentation for the procedure.
Network Plugin deployed?
Is Flannel using "eth0" interface for inter-host communication ?

https://kubernetes.io/docs/concepts/cluster-administration/addons/
https://github.com/flannel-io/flannel#deploying-flannel-manually

controlplane ~ ➜  vi kube-flannel.yml 
ocate the args section within the kube-flannel container definition. It should look like this:

  args:
  - --ip-masq
  - --kube-subnet-mgr
Add the additional argument - --iface=eth0 to the existing list of arguments.
----------

controlplane ~ ➜  kubectl apply -f kube-flannel.yml
namespace/kube-flannel unchanged
serviceaccount/flannel unchanged
clusterrole.rbac.authorization.k8s.io/flannel unchanged
clusterrolebinding.rbac.authorization.k8s.io/flannel unchanged
configmap/kube-flannel-cfg unchanged
daemonset.apps/kube-flannel-ds configured

controlplane ~ ➜  kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
namespace/kube-flannel created
serviceaccount/flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created

controlplane ~ ➜  kubectl get pod --all-namespaces 
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-d9mtf                  1/1     Running   0          4m1s   <----------
kube-flannel   kube-flannel-ds-q828p                  1/1     Running   0          3m57s  <----------
kube-system    coredns-6f6b679f8f-fmsnm               1/1     Running   0          22m
kube-system    coredns-6f6b679f8f-h8m7q               1/1     Running   0          22m
kube-system    etcd-controlplane                      1/1     Running   0          22m
kube-system    kube-apiserver-controlplane            1/1     Running   0          22m
kube-system    kube-controller-manager-controlplane   1/1     Running   0          22m
kube-system    kube-proxy-87n72                       1/1     Running   0          22m
kube-system    kube-proxy-cvgs7                       1/1     Running   0          13m
kube-system    kube-scheduler-controlplane            1/1     Running   0          22m

-------------------------
--- ORIGINAL SOLUTION ---
On the controlplane node, run the following set of commands to deploy the network plugin:

Download the original YAML file and save it as kube-flannel.yml:
curl -LO https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
Open the kube-flannel.yml file using a text editor.

Locate the args section within the kube-flannel container definition. It should look like this:

  args:
  - --ip-masq
  - --kube-subnet-mgr
Add the additional argument - --iface=eth0 to the existing list of arguments.

Now apply the modified manifest kube-flannel.yml file using kubectl:

kubectl apply -f kube-flannel.yml
After applying the manifest, the STATUS of both the nodes should become Ready

controlplane ~ ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   15m   v1.31.0
node01         Ready    <none>          15m   v1.31.0
------------------------------------------------------------------------------------
