Scheduling
--------------
alias gp='kubectl get pods'
alias gn='kubectl get nodes'
alias gs='kubectl get services'
alias grs='kubectl get replicasets'
alias gd='kubectl get deployments'
alias gns='kubectl get namespaces'
alias delp='kubectl delete pod'
alias deld='kubectl delete deployments'
alias descp='kubectl describe pod'
alias descn='kubectl describe node'
alias descd='kubectl describe deployment'
alias descrs='kubectl describe replicasets'
alias ga='kubectl get all'

******* MANUAL Sch ********************************************
****************************************************************
--------------------------------------------------
A pod definition file nginx.yaml is given. Create a pod using the file.
kubectl create -f nginx.yaml
--------------------------------------------------
What is the status of the created POD?
Pending
--------------------------------------------------
Why is the POD in a pending state?
kubectl describe pods nginx
kubectl describe nodes node01 | grep sched
Unschedulable:      false ---> No Scheduler present
kubectl get pods --namespace kube-system --> no scheduler
--------------------------------------------------
Manually schedule the pod on node01.
Delete and recreate the POD if necessary.
	vi nginx.yaml
	add "nodeName: node01" under spec:
Need this:
controlplane ~ ➜  cat nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  nodeName: node01
kubectl delete pod nginx && kubectl create -f nginx.yaml
--- or ---
kdp nginx && kubectl create -f nginx.yaml && sleep 4 && kgp
--------------------------------------------------
Now schedule the same pod on the controlplane node.
Delete and recreate the POD if necessary.
The ssame actions - only change on " controlplane "




***************************************************************************
******* MANUAL Sch ********************************************
****************************************************************
-----------------------------------------------------------------------
We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the "dev" environment (env)?
Use selectors to filter the output
gp -A --selector env=dev => 7
-----------------------------------------------------------------------
How many PODs are in the finance "business" unit (bu)?
gp -A --selector bu=finance => 6
-----------------------------------------------------------------------
How many objects are in the "prod" environment including PODs, ReplicaSets and any other objects?
ga --selector env=prod
-----------------------------------------------------------------------
dentify the POD which is part of the "prod" environment, the finance "BU" and of "frontend tier"?
descp app-1-zzxdf
kubectl get pods -l env=prod,bu=finance,tier=frontend
-----------------------------------------------------------------------
A ReplicaSet definition file is given replicaset-definition-1.yaml. Attempt to create the replicaset; you will encounter an issue with the file. Try to fix it.
Once you fix the issue, create the replicaset from the definition file.
cat replicaset-definition-1.yaml

--- Nedd to make the same labes
--- ORIGINAL:
controlplane ~ ➜  cat replicaset-definition-1.yaml 
apiVersion: apps/v1
kind: ReplicaSet
metadata:
   name: replicaset-1
spec:
   replicas: 2
   selector:
      matchLabels:
        tier: front-end
   template:
     metadata:
       labels:
        tier: nginx
     spec:
       containers:
       - name: nginx
         image: nginx
-----------------------------------------------------------------------









***************************************************************************
******* Taints and Tolerations ********************************************
***************************************************************************
Taints and Tolerations — These are labes are specified where pods cannot be placed.
23 / 5 000
Pods cannot be placed.
-----------------------------------------------------------------------
How many Nodes exist on the system?
   kubectl get nodes --> 2
-----------------------------------------------------------------------
Do any taints exist on node01 node?
descn node01 | grep -i taints
-----------------------------------------------------------------------
Create a taint on node01 with key of 'spray', value of 'mortein' and effect of 'NoSchedule'
   kubectl taint nodes node01 spray=mortein:NoSchedule
controlplane ~ ➜  descn node01 | grep -i taints
Taints:             spray=mortein:NoSchedule
--------------------------------------------------------------------------------------------
Create a new pod with the nginx image and pod name as mosquito.
vi mosq.yml

apiVersion: v1
kind: Pod
metadata: 
  name: mosquito
spec:
  containers:
  - name: ng-mosq
    image: nginx
--- or 
kubectl run --generator=run-pod/v1 mosquito --image=nginx

kubectl apply -f mosq.yml
--------------------------------------------------------------------------------------------
What is the state of the POD? -> Pending
--------------------------------------------------------------------------------------------
Why do you think the pod is in a pending state?
   cannot tolerate taint spray:mortein
--------------------------------------------------------------------------------------------
Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
kubectl run --generator=run-pod/v1 bee --image=nginx --dry-run -o yaml > bee-definition.yaml
   see file in directory
   kubectl create -f bee-definition.yaml
---- or
vi bee.yml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: bee
  labels:
    run: bee
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: bee
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
    - key: "spray"
      operator: "Equal"
      value: "mortein"
      effect: "NoSchedule"
status: {}

kubectl create -f bee.yaml
--------------------------------------------------------------------------------------------
Do you see any taints on controlplane node?
Yes - NoSchedule
--------------------------------------------------------------------------------------------
Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
	descn controlplane | grep -i taints
	Taints:             node-role.kubernetes.io/control-plane:NoSchedule
	kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
--- Removed Taint and mosquito = Then status became Running
--------------------------------------------------------------------------------------------
What is the state of the pod 'mosquito' now?
   kubectl get pods --> Running
--------------------------------------------------------------------------------------------
Which node is the POD 'mosquito' on now?
   kubectl get pods -o wide --> master








***************************************************************************
******* Practice Test - Node Affinity ********************************************
***************************************************************************

1. How many Labels exist on node node01?
   kubectl describe nodes node01 --> 3

2. What is the value set to the label beta.kubernetes.io/arch on node01?
   kubectl describe nodes node01 --> amd64

3. Apply a label color=blue to node node01
   kubectl label nodes node01 color=blue
4. Create a new deployment named blue with the nginx image and 3 replicas.
vi depl_blue.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    run: blue
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: blue
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: blue
    spec:
      containers:
      - image: nginx
        name: blue
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: color
                  operator: In
                  values:
                    - blue
status: {}
--------------------------------------------------------------------------------------------

5. Which nodes can the pods for the blue deployment be placed on?
Make sure to check taints on both nodes!
node01 и CP
--------------------------------------------------------------------------------------------
6. Set Node Affinity to the deployment to place the pods on node01 only.
Сделано выше
--------------------------------------------------------------------------------------------
7. Which nodes are the pods placed on now?
Node01
--------------------------------------------------------------------------------------------
8. Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.
vi depl_red.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    run: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      run: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        run: red
    spec:
      containers:
      - image: nginx
        name: red
        resources: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists

kubectl apply -f dep_red.yml
--- or 
kubectl create -f dep_red.yml







***************************************************************************
******* Node Affinity ********************************************
***************************************************************************
--------------------------------------------------------------------------------------------
1. A pod called rabbit is deployed. Identify the CPU requirements set on the Pod
in the current(default) namespace  
descp rabbit ---> 500m
--------------------------------------------------------------------------------------------
2. Delete the rabbit Pod. ---> delp rabbit
--------------------------------------------------------------------------------------------
3. Another pod called elephant has been deployed in the default namespace. It fails to get to a running state. Inspect this pod and identify the Reason why it is not running. ---> OOMkilled

descp elephant | grep -i reason
      Reason:       CrashLoopBackOff
      Reason:       OOMKilled ---> OOMkilled
Back-off restarting failed container mem-stress in pod elephant_default

The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.
--------------------------------------------------------------------------------------------
4. ok button
--------------------------------------------------------------------------------------------
5. The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.
Delete and recreate the pod if required. Do not modify anything other than the required fields.

YML
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-12-26T14:19:20Z"
  name: elephant
  namespace: default
  resourceVersion: "959"
  uid: ecc1252c-27f9-4927-9490-c5c9f96cc181
spec:
  containers:
  - args:
    - --vm
    - "1"
    - --vm-bytes
    - 15M
    - --vm-hang
    - "1"
    command:
    - stress
    image: polinux/stress
    imagePullPolicy: Always
    name: mem-stress
    resources:
      limits:
        memory: 10Mi


kubectl get po <pod name> -oyaml > test.yaml

controlplane ~ ✖ gp elephant -oyaml > el.yml

controlplane ~ ➜  cat el.yml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-12-26T14:19:20Z"
  name: elephant
  namespace: default
  resourceVersion: "1051"
  uid: ecc1252c-27f9-4927-9490-c5c9f96cc181
spec:
  containers:
  - args:
    - --vm
    - "1"
    - --vm-bytes
    - 15M
    - --vm-hang
    - "1"
    command:
    - stress
    image: polinux/stress
    imagePullPolicy: Always
    name: mem-stress
    resources:
      limits:
        memory: 10Mi
      requests:
        memory: 5Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-llxpk
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-llxpk
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-12-26T14:19:23Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2024-12-26T14:19:20Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-12-26T14:25:11Z"
    message: 'containers with unready status: [mem-stress]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-12-26T14:25:11Z"
    message: 'containers with unready status: [mem-stress]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-12-26T14:19:20Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://4005ea73027848e99e8b372f6a54a24fb6485f3ea1c006295f6c1befd7910a3f
    image: docker.io/polinux/stress:latest
    imageID: docker.io/polinux/stress@sha256:b6144f84f9c15dac80deb48d3a646b55c7043ab1d83ea0a697c09097aaad21aa
    lastState:
      terminated:
        containerID: containerd://4005ea73027848e99e8b372f6a54a24fb6485f3ea1c006295f6c1befd7910a3f
        exitCode: 1
        finishedAt: "2024-12-26T14:30:12Z"
        reason: OOMKilled
        startedAt: "2024-12-26T14:30:12Z"
    name: mem-stress
    ready: false
    restartCount: 7
    started: false
    state:
      waiting:
        message: back-off 5m0s restarting failed container=mem-stress pod=elephant_default(ecc1252c-27f9-4927-9490-c5c9f96cc181)
        reason: CrashLoopBackOff
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-llxpk
      readOnly: true
      recursiveReadOnly: Disabled
  hostIP: 192.168.20.139
  hostIPs:
  - ip: 192.168.20.139
  phase: Running
  podIP: 10.42.0.10
  podIPs:
  - ip: 10.42.0.10
  qosClass: Burstable
  startTime: "2024-12-26T14:19:20Z"

delp elephant
edit file ---> vi el.yml
kubectl create -f el.yml

--------------------------------------------------------------------------------------------
6. ok
--------------------------------------------------------------------------------------------
7. delete ---> delp elephant








1. How many DaemonSets are created in the cluster in all namespaces?
Check all namespaces
kubectl get daemonsets.apps -A
--- or 
kubectl get daemonsets --all-namespaces --> 2

2. Which namespace is the kube-proxy Daemonset created in?
kubectl get daemonsets --all-namespaces --> kube-system

3. Which of the below is a DaemonSet?
kube-flannel-ds (or weave-net)

4. On how many nodes are the pods scheduled by the DaemonSet kube-proxy
   kubectl get daemonsets --all-namespaces --> 1

5. What is the image used by the POD deployed by the kube-flannel-ds DaemonSet?
controlplane ~ ✖ kubectl describe daemonsets -n kube-flannel kube-flannel-ds | grep -i image
    Image:      docker.io/flannel/flannel-cni-plugin:v1.2.0
    Image:      docker.io/flannel/flannel:v0.23.0
    Image:      docker.io/flannel/flannel:v0.23.0 ---> flannel:v0.23.0

6. Deploy a DaemonSet for FluentD Logging, use the given specifications:
Name: elasticsearch
Namespace: kube-system
Image: registry.k8s.io/fluentd-elasticsearch:1.20

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
        - name: elasticsearch
          image: registry.k8s.io/fluentd-elasticsearch:1.20

vi ds.yml ( past example above)
controlplane ~ ➜  kubectl create -f ds.yml 
daemonset.apps/elasticsearch created







***************************************************************************
******* Static PODs ********************************************
***************************************************************************
Статические Pod управляются непосредственно демоном kubelet на определенном узле, без наблюдения за ними со стороны сервера API. В отличие от Pod, которые управляются плоскостью управления (например, Deployment); вместо этого kubelet наблюдает за каждым статическим Pod (и перезапускает его в случае сбоя).
Статические Pod всегда привязаны к одному Kubelet на определенном узле. Kubelet автоматически пытается создать зеркальный Pod на сервере API Kubernetes для каждого статического Pod. Это означает, что Pod, работающие на узле, видны на сервере API, но не могут управляться оттуда. Имена Pod будут дополнены именем хоста узла с начальным дефисом.
ps -aux | grep kubelet
cat /var/lib/kubelet/config.yaml | grep tatic
staticPodPath: /etc/kubernetes/manifests
--------------------------------------------------------------------------------------------

1. How many static pods exist in this cluster in all namespaces?
kubectl get pods --all-namespaces -o custom-columns=NAME:.metadata.name,CONTROLLER:.metadata.o
wnerReferences[].kind,NAMESPACE:.metadata.namespace --___-- and find where CONTROLLER column -eq Node
--- or 
kubectl get pods --all-namespaces -o json | jq -r '.items | map(select(.metadata.ownerReferences[]?.kind == "Node" ) | .metadata.name) | .[]'
etcd-minikube
kube-apiserver-minikube
kube-controller-manager-minikube
kube-scheduler-minikube
--- or 
kubectl get pods -A | grep controlp ---> 4


2. Which of the below components is NOT deployed as a static pod?
kubectl get pods -A | grep controlp ---> coredns -> is not exist in the list


3. Which of the below components is NOT deployed as a static POD?
kubectl get pods -A | grep controlp ---> kube-proxy -> is not exist in the list


4. On which nodes are the static pods created currently?
kubectl get pods -A -o wide | grep controlp ---> controlplane


5. What is the path of the directory holding the static pod definition files?
/etc/kubelet/manifests (управляются непосредственно демоном kubelet на определенном узле)


6. How many pod definition files are present in the manifests directory?
ll /etc/kubernetes/manifests/ ---> 4 (как и этих ПОДов в системе)


7. What is the docker image used to deploy the kube-api server as a static pod?
cat kube-apiserver.yaml | grep -i image
    image: registry.k8s.io/kube-apiserver:v1.31.0 ---> registry.k8s.io/kube-apiserver:v1.31.0
    imagePullPolicy: IfNotPresent


8. Create a static pod named static-busybox that uses the busybox image and the command sleep 1000

mkdir -p /etc/kubelet/manifests
vi /etc/kubelet/manifests/static-busybox.yml

apiVersion: v1
kind: Pod
metadata:
  name: static-busybox

spec:
  containers:
  - name: static-busybox
    image: busybox
    resources:
    command:
      - sleep 1000

kubectl create -f /etc/kubelet/manifests/static-busybox.yml
Из этой Dir не заработало.
Просто положил в /etc/kubelet/manifests/static-busybox.yml и само развернулось и прокатило


9. Edit the image on the static pod to use busybox:1.28.4
Name: static-busybox
Image: busybox:1.28.4

vi /etc/kubernetes/manifests/static-busybox.yml ---> change image
controlplane /etc/kubernetes/manifests ➜  kubectl apply -f /etc/kubelet/manifests/static-busybox.yml


10. We just created a new static pod named static-greenbox. Find it and delete it.
This question is a bit tricky. But if you use the knowledge you gained in the previous questions in this lab, you should be able to find the answer to it.

kubectl get pods  --all-namespaces | grep -i green
default        static-greenbox-node01                 1/1     Running             0               2m8s

controlplane /etc/kubernetes/manifests ➜  kubectl delete pod static-greenbox-node01 
pod "static-greenbox-node01" deleted

controlplane /etc/kubernetes/manifests ➜  kubectl get pods  --all-namespaces | grep -i green
default        static-greenbox-node01                 0/1     Pending            0               4s

controlplane /etc/kubernetes/manifests ➜  ll /etc/kubernetes/manifests
total 32
drwxrwxr-x 1 root root 4096 Dec 27 11:31 ./
drwxrwxr-x 1 root root 4096 Dec 27 10:54 ../
-rw------- 1 root root 2565 Dec 27 10:54 etcd.yaml
-rw------- 1 root root 3898 Dec 27 10:54 kube-apiserver.yaml
-rw------- 1 root root 3394 Dec 27 10:54 kube-controller-manager.yaml
-rw-r--r-- 1 root root    0 Aug 13 08:22 .kubelet-keep
-rw------- 1 root root 1463 Dec 27 10:54 kube-scheduler.yaml
-rw-r--r-- 1 root root  177 Dec 27 11:31 static-busybox.yml

controlplane /etc/kubernetes/manifests ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   43m   v1.31.0
node01         Ready    <none>          43m   v1.31.0

controlplane /etc/kubernetes/manifests ➜  ssh node01

ps -aux | grep kubelet
root       21735  0.0  0.1 2931236 76180 ?       Ssl  11:32   0:04 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.10
root       26276  0.0  0.0   6800  2512 pts/0    S+   11:40   0:00 grep kubelet

node01 ~ ➜  cat /var/lib/kubelet/config.yaml
staticPodPath: /etc/just-to-mess-with-you
node01 ~ ➜  ls -l /etc/just-to-mess-with-you
total 4
-rw-r--r-- 1 root root 301 Dec 27 11:32 greenbox.yaml

node01 ~ ➜  rm /etc/just-to-mess-with-you/greenbox.yaml 

node01 ~ ➜  exit
logout
Connection to node01 closed.

controlplane /etc/kubernetes/manifests ✖ kubectl get pods  --all-namespaces | grep -i green

controlplane /etc/kubernetes/manifests ✖ 











***************************************************************************
******* Multiple Schedulers ********************************************
***************************************************************************
--------------------------------------------------------------------------------------------
1. What is the name of the POD that deploys the default kubernetes scheduler in this environment?
gp -A
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE
kube-flannel   kube-flannel-ds-96jfz                  1/1     Running   0          3m14s
kube-system    coredns-77d6fd4654-9s4j7               1/1     Running   0          3m13s
kube-system    coredns-77d6fd4654-vc6b5               1/1     Running   0          3m13s
kube-system    etcd-controlplane                      1/1     Running   0          3m18s
kube-system    kube-apiserver-controlplane            1/1     Running   0          3m18s
kube-system    kube-controller-manager-controlplane   1/1     Running   0          3m18s
kube-system    kube-proxy-46cr5                       1/1     Running   0          3m14s
kube-system    kube-scheduler-controlplane            1/1     Running   0          3m18s
--> kube-scheduler-controlplane


2. What is the image used to deploy the kubernetes scheduler?
descp -n kube-system kube-scheduler-controlplane | grep -i image
    Image:         registry.k8s.io/kube-scheduler:v1.31.0

3. We have already created the ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of.
Checkout the following Kubernetes objects:
ServiceAccount: my-scheduler (kube-system namespace)
ClusterRoleBinding: my-scheduler-as-kube-scheduler
ClusterRoleBinding: my-scheduler-as-volume-scheduler
Run the command: kubectl get serviceaccount -n kube-system and kubectl get clusterrolebinding
Note: - Don't worry if you are not familiar with these resources. We will cover it later on.

4. Let's create a configmap that the new scheduler will employ using the concept of ConfigMap as a volume.
We have already given a configMap definition file called my-scheduler-configmap.yaml at /root/ path that will create a configmap with name my-scheduler-config using the content of file /root/my-scheduler-config.yaml.
kubectl create -f /root/my-scheduler-configmap.yaml

controlplane ~ ➜  ll /root/my*
-rw-r--r-- 1 root root 336 Dec 27 11:46 /root/my-scheduler-configmap.yaml
-rw-r--r-- 1 root root 155 Dec  3 11:24 /root/my-scheduler-config.yaml
-rw-r--r-- 1 root root 893 Dec  3 11:24 /root/my-scheduler.yaml

--------- controlplane ~ ➜  cat /root/my-scheduler.yaml 
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: my-scheduler
  name: my-scheduler
  namespace: kube-system
spec:
  serviceAccountName: my-scheduler
  containers:
  - command:
    - /usr/local/bin/kube-scheduler
    - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml
    image: <use-correct-image>
    livenessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 15
    name: kube-second-scheduler
    readinessProbe:
      httpGet:
        path: /healthz
        port: 10259
        scheme: HTTPS
    resources:
      requests:
        cpu: '0.1'
    securityContext:
      privileged: false
    volumeMounts:
      - name: config-volume
        mountPath: /etc/kubernetes/my-scheduler
  hostNetwork: false
  hostPID: false
  volumes:
    - name: config-volume
      configMap:
        name: my-scheduler-config  


------------ controlplane ~ ➜  cat /root/my-scheduler-config.yaml 
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler
leaderElection:
  leaderElect: false


----------- controlplane ~ ➜  cat /root/my-scheduler-configmap.yaml 
apiVersion: v1
data:
  my-scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
      - schedulerName: my-scheduler
    leaderElection:
      leaderElect: false
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: my-scheduler-config
  namespace: kube-system


5. Deploy an additional scheduler to the cluster following the given specification.
Use the manifest file provided at /root/my-scheduler.yaml. Use the same image as used by the default kubernetes scheduler.

Name: my-scheduler
Status: Running
Correct image used?

controlplane ~ ➜  kubectl create -f /root/my-scheduler.yaml 
pod/my-scheduler created

Создается но не работает т.к. в конфиге не указан образ, директива есть но не указан образ.
controlplane ~ ➜  kubectl get pod -A | grep -i sche
kube-system    kube-scheduler-controlplane            1/1     Running            0          18m
kube-system    my-scheduler                           0/1     InvalidImageName   0          3m31s


Получаем из работающего планировщика образ:
controlplane ~ ➜  kubectl get pod -n kube-system kube-scheduler-controlplane -oyaml | grep -i image
    image: registry.k8s.io/kube-scheduler:v1.31.0
Опять vi /root/my-scheduler.yaml и втыкаем в image registry.k8s.io/kube-scheduler:v1.31.0

Далее kubectl apply -f /root/my-scheduler.yaml и controlplane ~ ➜  kubectl get pod -A | grep -i my-sche
kube-system    my-scheduler     1/1     Running   0          5m39s


6. A POD definition file is given. Use it to create a POD with the new custom scheduler.
File is located at /root/nginx-pod.yaml

Uses custom scheduler
Status: Running

cat /root/nginx-pod.yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: nginx 
spec:
  containers:
  - image: nginx
    name: nginx

vi /root/nginx-pod.yaml --> add schedulerName: my-scheduler under spec
   kubectl create -f /root/nginx-pod.yaml

---> 
apiVersion: v1
kind: Pod 
metadata:
  name: nginx
spec:
  schedulerName: my-scheduler
  containers:
    #  - image: nginx
  - image: nginx
    name: nginx
