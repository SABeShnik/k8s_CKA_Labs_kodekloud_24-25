2_Cluster_Upgrade_Process

1. This lab tests your skills on upgrading a kubernetes cluster. 
We have a production cluster with applications running on it. Let us explore the setup first.
What is the current version of the cluster?

controlplane ~ ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   20m   v1.30.0
node01         Ready    <none>          19m   v1.30.0
---> v1.30.0
------------------------------------------------------------------------

2. How many nodes are part of this cluster?
Including controlplane and worker nodes ---> 2
------------------------------------------------------------------------

3. How many nodes can host workloads in this cluster?
Inspect the applications and taints set on the nodes.

controlplane ~ ➜  kubectl get pods -o wide
NAME                   READY   STATUS    RESTARTS   AGE    IP           NODE           NOMINATED NODE   READINESS GATES
blue-fffb6db8d-4kqfw   1/1     Running   0          5m7s   10.244.1.3   node01         <none>           <none>
blue-fffb6db8d-ffnlm   1/1     Running   0          5m7s   10.244.1.4   node01         <none>           <none>
blue-fffb6db8d-jgn2b   1/1     Running   0          5m7s   10.244.1.2   node01         <none>           <none>
blue-fffb6db8d-jl8nx   1/1     Running   0          5m7s   10.244.0.4   controlplane   <none>           <none>
blue-fffb6db8d-svv58   1/1     Running   0          5m7s   10.244.0.5   controlplane   <none>           <none>
---> 2
------------------------------------------------------------------------

4. How many applications are hosted on the cluster?
Count the number of deployments in the default namespace.

controlplane ~ ➜  kubectl get deployments.apps 
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   5/5     5            5           6m19s ---> 1
------------------------------------------------------------------------

5. What nodes are the pods hosted on? 
controlplane ~ ➜  kubectl get pods -o wide ---> controlplane + node01
------------------------------------------------------------------------

6. DUMM Answer (if cluster will fall down while upgrade process?) You are tasked to upgrade the cluster. Users accessing the applications must not be impacted, 
and you cannot provision new VMs. What strategy would you use to upgrade the cluster?

---> Upgrade one node at a time while moving the workloads to the other
------------------------------------------------------------------------

7. What is the latest version available for an upgrade with the current version of the kubeadm tool installed?
Use the kubeadm tool

controlplane ~ ➜  kubeadm upgrade plan
COMPONENT   NODE           CURRENT   TARGET
kubelet     controlplane   v1.30.0   v1.30.8
kubelet     node01         v1.30.0   v1.30.8 ---> v1.30.8
------------------------------------------------------------------------

8. We will be upgrading the controlplane node first. Drain the controlplane node of workloads and mark it UnSchedulable
controlplane ~ ➜  gp -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
blue-fffb6db8d-kp6nj   1/1     Running   0          14m   10.244.0.5   controlplane   <none>           <none>
blue-fffb6db8d-kt6zm   1/1     Running   0          14m   10.244.1.3   node01         <none>           <none>
blue-fffb6db8d-ktps7   1/1     Running   0          14m   10.244.1.4   node01         <none>           <none>
blue-fffb6db8d-pngmz   1/1     Running   0          14m   10.244.0.4   controlplane   <none>           <none>
blue-fffb6db8d-xxgkd   1/1     Running   0          14m   10.244.1.2   node01         <none>           <none>

controlplane ~ ➜  kubectl drain controlplane --ignore-daemonsets
node/node01 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-fg9wg, kube-system/kube-proxy-9ql47
evicting pod default/blue-fffb6db8d-xxgkd
evicting pod default/blue-fffb6db8d-kt6zm
evicting pod default/blue-fffb6db8d-ktps7
pod/blue-fffb6db8d-kt6zm evicted
pod/blue-fffb6db8d-xxgkd evicted
pod/blue-fffb6db8d-ktps7 evicted
node/controlplane drained

controlplane ~ ➜  gp -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
blue-fffb6db8d-cs4md   1/1     Running   0          65s   10.244.1.12   node01   <none>           <none>
blue-fffb6db8d-h8rh5   1/1     Running   0          65s   10.244.1.9    node01   <none>           <none>
blue-fffb6db8d-p4prd   1/1     Running   0          65s   10.244.1.10   node01   <none>           <none>
blue-fffb6db8d-s4l7z   1/1     Running   0          65s   10.244.1.11   node01   <none>           <none>
blue-fffb6db8d-tqc6m   1/1     Running   0          66s   10.244.1.8    node01   <none>           <none>
------------------------------------------------------------------------

9. Upgrade the controlplane components to exact version v1.31.0
Upgrade the kubeadm tool (if not already), then the controlplane components, and finally the kubelet. 
Practice referring to the Kubernetes documentation page.
https://v1-31.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/
apt-get update ---> apt-get upgrade kubeadm ---> kubeadm upgrade plan

controlplane ~ ✖ kubeadm upgrade plan
[preflight] Running pre-flight checks.
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[upgrade] Running cluster health checks
[upgrade/health] FATAL: [preflight] Some fatal errors occurred:
        [ERROR CreateJob]: Job "upgrade-health-check-jcfz7" in the namespace "kube-system" did not complete in 15s: no condition of type Complete

Once again:
COMPONENT   NODE           CURRENT   TARGET
kubelet     controlplane   v1.30.0   v1.30.8
kubelet     node01         v1.30.0   v1.30.8

controlplane ~ ➜  vi /etc/apt/sources.list.d/kubernetes.list
add string: deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /
controlplane ~ ➜  apt update

controlplane ~ ➜ sudo apt-get install -y kubeadm='1.31.0-1.1' ---> Preparing to unpack .../kubeadm_1.31.0-1.1_amd64.deb ...
controlplane ~ ➜  kubeadm version ---> kubeadm version: "1""31"
controlplane ~ ➜  sudo apt-get install -y kubelet='1.31.0-1.1'
controlplane ~ ➜ kubelet --version ---> Kubernetes v1.31.0

controlplane ~ ➜  kubeadm upgrade plan
COMPONENT   NODE           CURRENT   TARGET
kubelet     controlplane   v1.30.0   v1.31.4
kubelet     node01         v1.30.0   v1.31.4

controlplane ~ ➜  apt-cache madison kubeadm
   kubeadm | 1.31.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
   kubeadm | 1.31.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
   kubeadm | 1.31.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
   kubeadm | 1.31.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
   kubeadm | 1.31.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
   kubeadm | 1.30.8-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.7-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.6-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.5-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages
   kubeadm | 1.30.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb  Packages


sudo kubeadm upgrade apply v1.31.0
[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.31.0". Enjoy!
[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
controlplane ~ ➜  kubectl get nodes -o wide ---> 1.30.0



controlplane ~ ➜  sudo systemctl daemon-reload && sudo systemctl restart kubelet
controlplane ~ ✖ kubectl get nodes -o wide
NAME           STATUS                     ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready,SchedulingDisabled   control-plane   30m   v1.31.0   192.6.241.12   <none>        Ubuntu 22.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.26
node01         Ready                      <none>          29m   v1.30.0   192.6.241.3    <none>        Ubuntu 22.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.26




--- or may try 
apt install -y kubeadm=1.31.0-1.1
   kubeadm upgrade apply 1.31.0
   apt install -y kubelet=1.31.0-1.1
   systemctl restart kubelet
   


#sudo apt-mark unhold kubeadm && \
#sudo apt-get update && sudo apt-get install -y kubeadm='1.31.0-1.1' && \
#sudo apt-mark hold kubeadm
--------------------------------------------------------------------------------------------
------------------------------------------------------------------------


10. Mark the controlplane node as "Schedulable" again
controlplane ~ ➜  kubectl uncordon controlplane
node/controlplane uncordoned
------------------------------------------------------------------------


11. Next is the worker node. Drain the worker node of the workloads and mark it UnSchedulable
controlplane ~ ✖ kubectl drain node01 --ignore-daemonsets
node/node01 already cordoned
------------------------------------------------------------------------


12. Upgrade the worker node to the exact version v1.31.0
controlplane ~ ➜  kubectl get nodes -o wide
NAME           STATUS                     ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready                      control-plane   34m   v1.31.0   192.6.241.12   <none>        Ubuntu 22.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.26
node01         Ready,SchedulingDisabled   <none>          33m   v1.30.0   192.6.241.3    <none>        Ubuntu 22.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.26

controlplane ~ ➜  ssh node01
controlplane ~ ➜  vi /etc/apt/sources.list.d/kubernetes.list
add string: deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /
controlplane ~ ➜  apt update

node01 ~ ➜ sudo apt-mark unhold kubeadm ---> Canceled hold on kubeadm.
node01 ~ ➜  apt install -y kubeadm=1.31.0-1.1 ---> Unpacking kubeadm (1.31.0-1.1) over (1.30.0-1.1) ...
node01 ~ ➜  kubeadm version ---> 1.31.0-1.1

node01 ~ ✖ sudo apt-mark unhold kubelet ---> Canceled hold on kubelet.
node01 ~ ➜  apt install -y kubelet=1.31.0-1.1 ---> Unpacking kubelet (1.31.0-1.1) over (1.30.0-1.1) ...
node01 ~ ➜  kubelet --version ---> Kubernetes v1.31.0

sudo kubeadm upgrade node
sudo systemctl daemon-reload && sudo systemctl restart kubelet ---> exit ssh ---> on controlplane:
controlplane ~ ✖ kubectl get nodes -o wide
NAME           STATUS                     ROLES           AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
controlplane   Ready                      control-plane   51m   v1.31.0   192.6.241.12   <none>        Ubuntu 22.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.26
node01         Ready,SchedulingDisabled   <none>          50m   v1.31.0   192.6.241.3    <none>        Ubuntu 22.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.26
------------------------------------------------------------------------

13. Remove the restriction and mark the worker node as schedulable again.
controlplane ~ ➜  kubectl uncordon node01
node/node01 uncordoned
------------------------------------------------------------------------

